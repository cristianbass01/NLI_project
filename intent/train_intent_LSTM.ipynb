{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import BertModel\n",
    "from datasets import load_dataset\n",
    "from torch import nn\n",
    "import spacy\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import f_classif, SelectKBest\n",
    "import string\n",
    "import fasttext\n",
    "from sklearn.svm import SVC\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "import pickle\n",
    "\n",
    "BERT_TOKENIZER = True\n",
    "\n",
    "if not BERT_TOKENIZER:\n",
    "    embedder = fasttext.load_model('fasttext/cc.en.300.bin')\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "else:\n",
    "    model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "    embedding_matrix = model.embeddings.word_embeddings.weight\n",
    "    transformer_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self, input_size, num_cells, hidden_size, bi, out_features):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size = input_size, num_layers = num_cells, hidden_size = hidden_size, bidirectional=bi, batch_first = True)\n",
    "        self.batch_norm = nn.BatchNorm1d(num_features = hidden_size * 2 if bi else hidden_size)\n",
    "        self.dropout = nn.Dropout(p = 0.2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.linear1 = nn.Linear(in_features = hidden_size * 2 if bi else hidden_size, out_features = out_features)\n",
    "        #self.relu2 = nn.ReLU()\n",
    "        #self.linear2 = nn.Linear(in_features = 100, out_features = out_features)\n",
    "        \n",
    "        # with torch.no_grad():\n",
    "            # self.linear1.bias.fill_(-torch.log(torch.tensor(out_features - 1)))\n",
    "            # self.linear2.bias.fill_(-torch.log(torch.tensor(out_features - 1)))\n",
    "    \n",
    "    def forward(self, embedding_sequence):\n",
    "        # Pad first sequence to max length\n",
    "        # embedding_sequence[0] = torch.concat([embedding_sequence[0], torch.zeros((self.max_sequence_length - embedding_sequence[0].shape[0] ,self.input_size)).cuda()])\n",
    "        # Get lenghts vector for every embeddings sequence to later use for packing\n",
    "        lengths = torch.Tensor([embedding.shape[0] for embedding in embedding_sequence]).long()\n",
    "        # Pad sequence\n",
    "        padded_sequence = pad_sequence(embedding_sequence)\n",
    "        # Pack sequence\n",
    "        packed_sequence = pack_padded_sequence(padded_sequence, lengths = lengths, enforce_sorted = False)\n",
    "        # print(padded_sequence.shape)\n",
    "        \n",
    "        packed_out, _ = self.lstm(packed_sequence)\n",
    "        padded_out, _ = pad_packed_sequence(packed_out)\n",
    "    \n",
    "        # print(padded_out.shape)\n",
    "    \n",
    "        out_forward = padded_out[lengths - 1, range(padded_out.shape[1]), :self.hidden_size]\n",
    "        out_reverse = padded_out[0, :, self.hidden_size:]\n",
    "        \n",
    "        # print(out_forward.shape)\n",
    "        # print(out_reverse.shape)\n",
    "        \n",
    "        out = torch.cat([out_forward, out_reverse], dim = 1)\n",
    "        \n",
    "        # print(out.shape)\n",
    "        \n",
    "        x = self.batch_norm(out)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear1(x)\n",
    "        #x = self.relu2(x)\n",
    "        #x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_features = 768 if BERT_TOKENIZER else 300\n",
    "batch_size = 32\n",
    "learning_rate = 0.0001\n",
    "epochs = 50\n",
    "patience = 2\n",
    "class_weight_beta = 0.9999\n",
    "use_history = True\n",
    "\n",
    "TRANSFORMER_MODEL_NAME = 'roberta-base' # ignore this for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_intent_list(intent_list):\n",
    "    intents = set()\n",
    "    if len(intent_list) == 0:\n",
    "        intents.add('other')\n",
    "    for intent in intent_list:\n",
    "        if intent.startswith('Restaurant'):\n",
    "            intents.add(intent)\n",
    "        elif intent.startswith('Hotel'):\n",
    "            intents.add(intent)\n",
    "        elif intent.startswith('general'):\n",
    "            intents.add(intent)\n",
    "        else:\n",
    "            intents.add('other')\n",
    "    # print(f'Original {intent_list}')\n",
    "    # print(f'Modified {list(intents)}')\n",
    "    return list(intents)\n",
    "\n",
    "def preprocess_split(dataset, split):\n",
    "    df = dataset[split].to_pandas()\n",
    "    new_df = pd.DataFrame(columns = df.columns)\n",
    "    for i in range(len(df)):\n",
    "        # Taken from notebook, to know which lines to skip\n",
    "        row = df.loc[i]\n",
    "        if not any(set(row.turns['frames'][turn_id]['service']).intersection(['hotel', 'restaurant']) for turn_id,utt in enumerate(row.turns['utterance'])):\n",
    "            continue\n",
    "        new_df.loc[len(new_df)] = row\n",
    "        # new_df.loc[len(new_df) - 1]['services'] = process_service_list(new_df.loc[len(new_df) - 1]['services'])\n",
    "        # for i, frame_service in [frame['service'] for frame in df.loc[i].turns['frames']]:\n",
    "            # df.loc[i].turns['frames']\n",
    "    return new_df\n",
    "\n",
    "def extract_feature_df(dataset):\n",
    "    act_types = []\n",
    "    utterance_list = []\n",
    "    embedding_list = []\n",
    "    \n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        turns = dataset.loc[i].turns\n",
    "        # print(dataset.loc[i].turns['utterance'])\n",
    "        # print([frame['service'] for frame in dataset.loc[i].turns['frames']])\n",
    "        for j, (utterance, speaker, dialogue_act) in enumerate(zip(turns['utterance'], turns['speaker'], turns['dialogue_acts'])):\n",
    "            if speaker != 0: # if it's the user's turn\n",
    "                continue\n",
    "            if j == 0:\n",
    "                prev_user_utterance = ''\n",
    "                prev_user_acts = []\n",
    "                prev_bot_utterance = ''\n",
    "                prev_bot_acts = []\n",
    "            else:\n",
    "                prev_user_utterance = turns['utterance'][j - 2]\n",
    "                prev_user_acts = process_intent_list(turns['dialogue_acts'][j - 2]['dialog_act']['act_type'])\n",
    "                prev_bot_utterance = turns['utterance'][j - 1]\n",
    "                prev_bot_acts = process_intent_list(turns['dialogue_acts'][j - 1]['dialog_act']['act_type'])\n",
    "            \n",
    "            if use_history:\n",
    "                utterance = ' | '.join([prev_user_utterance, ', '.join(prev_user_acts), prev_bot_utterance, ', '.join(prev_bot_acts), utterance])\n",
    "            \n",
    "            if not BERT_TOKENIZER:\n",
    "                utterance = utterance.translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation)))\n",
    "            act_type = dialogue_act['dialog_act']['act_type']\n",
    "            new_utterance = []\n",
    "            word_embedding_list = []\n",
    "            \n",
    "            #### Spacy tokenization + fasttext\n",
    "            if not BERT_TOKENIZER:\n",
    "                doc = nlp(utterance)\n",
    "                for token in doc:\n",
    "                    new_utterance.append(token.lemma_)\n",
    "                    word_embedding_list.append(embedder.get_word_vector(token.lemma_))\n",
    "                    embedder\n",
    "                embedding = np.stack(word_embedding_list)\n",
    "            #### Bert embedder\n",
    "            else:\n",
    "                tokenized = transformer_tokenizer(utterance)\n",
    "                with torch.no_grad():\n",
    "                    embedding = embedding_matrix[tokenized.input_ids].detach().numpy()\n",
    "            ####\n",
    "            \n",
    "            # np.mean(word_embedding_list, axis = 0)\n",
    "            \n",
    "            new_utterance = ' '.join(new_utterance)\n",
    "            \n",
    "            embedding_list.append(embedding)\n",
    "            act_types.append(process_intent_list(act_type))\n",
    "            utterance_list.append(new_utterance)\n",
    "                    \n",
    "    return utterance_list, embedding_list, act_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: multi_woz_v22/v2.2_active_only\n",
      "Found cached dataset multi_woz_v22 (/home/adrian/.cache/huggingface/datasets/multi_woz_v22/v2.2_active_only/2.2.0/6719c8b21478299411a0c6fdb7137c3ebab2e6425129af831687fb7851c69eb5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b44909eafe714320ac6e3792a04cb9c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6321/6321 [00:13<00:00, 451.77it/s]\n",
      "100%|██████████| 745/745 [00:02<00:00, 330.20it/s]\n",
      "100%|██████████| 745/745 [00:02<00:00, 313.30it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('multi_woz_v22')\n",
    "\n",
    "try:\n",
    "    train\n",
    "    print(\"Dataset already loaded, moving on\")\n",
    "except:\n",
    "    train = preprocess_split(dataset, 'train')\n",
    "    test = preprocess_split(dataset, 'test')\n",
    "    val = preprocess_split(dataset, 'validation')\n",
    "    train_utterance_list, train_embedding_list, train_act_type = extract_feature_df(train)\n",
    "    test_utterance_list, test_embedding_list, test_act_type = extract_feature_df(test)\n",
    "    val_utterance_list, val_embedding_list, val_act_type = extract_feature_df(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'general-thank', 'general-greet', 'Hotel-Request', 'Restaurant-Request', 'other', 'Hotel-Inform', 'Restaurant-Inform', 'general-bye'}\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(set(act for acts in train_act_type for act in acts))\n",
    "mlb = MultiLabelBinarizer().fit(train_act_type)\n",
    "pickle.dump(mlb, open('saved_models/mlb.pkl', 'wb'))\n",
    "model = MyLSTM(input_size = nr_features, num_cells = 4, hidden_size = 300, bi = True, out_features = len(mlb.classes_)).cuda()\n",
    "model.train()\n",
    "\n",
    "train_labels = mlb.transform(train_act_type)\n",
    "val_labels = mlb.transform(val_act_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts:\n",
      "[('Hotel-Inform', 11521), ('Hotel-Request', 2107), ('Restaurant-Inform', 11063), ('Restaurant-Request', 2719), ('general-bye', 2052), ('general-greet', 78), ('general-thank', 5247), ('other', 13894)]\n",
      "Class weights:\n",
      "[('Hotel-Inform', 0.07781679375321442), ('Hotel-Request', 0.28017169940200043), ('Restaurant-Inform', 0.07953861567289042), ('Restaurant-Request', 0.22358140714587124), ('general-bye', 0.28691842694505404), ('general-greet', 6.8506983696367305), ('general-thank', 0.1303759509443874), ('other', 0.07089873649985191)]\n"
     ]
    }
   ],
   "source": [
    "samples_per_class = [0] * len(mlb.classes_)\n",
    "for act_types in train_act_type:\n",
    "    for act_type in act_types:\n",
    "        samples_per_class[np.argmax(mlb.transform([[act_type]]))] += 1\n",
    "\n",
    "print(\"Class counts:\")\n",
    "print([*zip(mlb.classes_, samples_per_class)])\n",
    "\n",
    "samples_per_class = np.array(samples_per_class)\n",
    "\n",
    "effective_num = 1.0 - np.power(class_weight_beta, samples_per_class)\n",
    "class_weights = (1.0 - class_weight_beta) / effective_num\n",
    "class_weights = class_weights / np.sum(class_weights) * len(mlb.classes_)\n",
    "print(\"Class weights:\")\n",
    "print([*zip(mlb.classes_, class_weights)])\n",
    "class_weights = torch.Tensor(class_weights).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify_tokens_tags(embedding_list, labels_list, batch_size):\n",
    "    embeddings_batch = []\n",
    "    labels_batch = []\n",
    "    \n",
    "    if labels_list is None:\n",
    "        labels_list = range(len(embedding_list))\n",
    "    \n",
    "    for embeddings, label in zip(embedding_list, labels_list):\n",
    "        embeddings_batch.append(torch.Tensor(embeddings).cuda())\n",
    "        labels_batch.append(label)\n",
    "        \n",
    "        if len(embeddings_batch) == batch_size:\n",
    "            yield embeddings_batch, torch.Tensor(labels_batch).cuda()\n",
    "            embeddings_batch.clear()\n",
    "            labels_batch.clear()\n",
    "    \n",
    "    yield embeddings_batch, torch.Tensor(labels_batch).cuda()\n",
    "    return None\n",
    "\n",
    "def compute_loss(model, embedding_list, labels_list, batch_size, criterion):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for embeddings_batch, labels_batch in batchify_tokens_tags(embedding_list, labels_list, batch_size):\n",
    "            out = model.forward(embeddings_batch)\n",
    "            \n",
    "            loss = criterion(out, labels_batch)\n",
    "            losses.append(loss.item())\n",
    "    model.train()\n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "criterion = nn.BCEWithLogitsLoss(weight = class_weights)\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "waited = 0\n",
    "\n",
    "min_val_loss = np.inf\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_train_loss = []\n",
    "    \n",
    "    for embeddings_batch, labels_batch in batchify_tokens_tags(train_embedding_list, train_labels, batch_size):\n",
    "        optim.zero_grad()\n",
    "        out = model.forward(embeddings_batch)\n",
    "        \n",
    "        # logits_final, labels_final = outputs_keep_useful_part(out.logits, labels_batch, useful_pos_batch)\n",
    "        loss = criterion(out, labels_batch)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        epoch_train_loss.append(loss.item())\n",
    "    \n",
    "    epoch_train_loss = np.mean(epoch_train_loss)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    epoch_val_loss = compute_loss(model, val_embedding_list, val_labels, batch_size, criterion)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}: Train loss = {epoch_train_loss}, Val loss = {epoch_val_loss}\")\n",
    "    \n",
    "    if epoch_val_loss < min_val_loss:\n",
    "        min_val_loss = epoch_val_loss\n",
    "        torch.save(model.state_dict(), 'saved_models/INT_LSTM_CORRECT1.pt')\n",
    "    \n",
    "    if len(val_losses) != 0 and val_losses[-1] <= epoch_val_loss:\n",
    "        waited += 1\n",
    "        if waited > patience:\n",
    "                val_losses.append(epoch_val_loss)\n",
    "                break\n",
    "    else:\n",
    "        waited = 0\n",
    "    \n",
    "    val_losses.append(epoch_val_loss)\n",
    "\n",
    "plt.plot(train_losses)\n",
    "plt.plot(val_losses)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, embeddings_list, batch_size):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for embeddings_batch, _ in batchify_tokens_tags(embeddings_list, None, batch_size):\n",
    "            out = model.forward(embeddings_batch)\n",
    "            predictions.append((out > 0.5).cpu().detach().numpy())\n",
    "    return np.concatenate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py\", line 3457, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_4237/1803708191.py\", line 1, in <module>\n",
      "    model.load_state_dict(torch.load('saved_models/INT_LSTM.pt'))\n",
      "  File \"/home/adrian/.local/lib/python3.10/site-packages/torch/serialization.py\", line 771, in load\n",
      "    with _open_file_like(f, 'rb') as opened_file:\n",
      "  File \"/home/adrian/.local/lib/python3.10/site-packages/torch/serialization.py\", line 270, in _open_file_like\n",
      "    return _open_file(name_or_buffer, mode)\n",
      "  File \"/home/adrian/.local/lib/python3.10/site-packages/torch/serialization.py\", line 251, in __init__\n",
      "    super(_open_file, self).__init__(open(name, mode))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'saved_models/INT_LSTM.pt'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py\", line 2077, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'FileNotFoundError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/usr/lib/python3/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/lib/python3/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n",
      "    module = getmodule(object, filename)\n",
      "  File \"/usr/lib/python3.10/inspect.py\", line 878, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/usr/lib/python3.10/posixpath.py\", line 396, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, strict, {})\n",
      "  File \"/usr/lib/python3.10/posixpath.py\", line 431, in _joinrealpath\n",
      "    st = os.lstat(newpath)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4237/1803708191.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'saved_models/INT_LSTM.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_act_type_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_embedding_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'saved_models/INT_LSTM.pt'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2076\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2077\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2078\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FileNotFoundError' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2077\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2078\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2079\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2080\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('saved_models/INT_LSTM_CORRECT1.pt'))\n",
    "\n",
    "test_act_type_pred = predict(model, test_embedding_list, batch_size)\n",
    "\n",
    "# print(mlb.transform(test_act_type).shape)\n",
    "# print(test_act_type_pred.shape)\n",
    "\n",
    "acc = accuracy_score(mlb.transform(test_act_type), test_act_type_pred)\n",
    "report = classification_report(mlb.transform(test_act_type), test_act_type_pred, target_names = mlb.classes_, digits = 3)\n",
    "print(report)\n",
    "print(f'acc = {acc}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
