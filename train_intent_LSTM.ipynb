{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import BertModel\n",
    "from datasets import load_dataset\n",
    "from torch import nn\n",
    "import spacy\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import f_classif, SelectKBest\n",
    "import string\n",
    "import fasttext\n",
    "from sklearn.svm import SVC\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "\n",
    "BERT_TOKENIZER = True\n",
    "\n",
    "if not BERT_TOKENIZER:\n",
    "    embedder = fasttext.load_model('fasttext/cc.en.300.bin')\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "else:\n",
    "    model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "    embedding_matrix = model.embeddings.word_embeddings.weight\n",
    "    transformer_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self, input_size, num_cells, hidden_size, bi, out_features):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size = input_size, num_layers = num_cells, hidden_size = hidden_size, bidirectional=bi, batch_first = True)\n",
    "        self.batch_norm = nn.BatchNorm1d(num_features = hidden_size * 2 if bi else hidden_size)\n",
    "        self.dropout = nn.Dropout(p = 0.2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.linear1 = nn.Linear(in_features = hidden_size * 2 if bi else hidden_size, out_features = out_features)\n",
    "        #self.relu2 = nn.ReLU()\n",
    "        #self.linear2 = nn.Linear(in_features = 100, out_features = out_features)\n",
    "        \n",
    "        # with torch.no_grad():\n",
    "            # self.linear1.bias.fill_(-torch.log(torch.tensor(out_features - 1)))\n",
    "            # self.linear2.bias.fill_(-torch.log(torch.tensor(out_features - 1)))\n",
    "    \n",
    "    def forward(self, embedding_sequence):\n",
    "        # Pad first sequence to max length\n",
    "        # embedding_sequence[0] = torch.concat([embedding_sequence[0], torch.zeros((self.max_sequence_length - embedding_sequence[0].shape[0] ,self.input_size)).cuda()])\n",
    "        # Get lenghts vector for every embeddings sequence to later use for packing\n",
    "        lengths = torch.Tensor([embedding.shape[0] for embedding in embedding_sequence]).long()\n",
    "        # Pad sequence\n",
    "        padded_sequence = pad_sequence(embedding_sequence)\n",
    "        # Pack sequence\n",
    "        packed_sequence = pack_padded_sequence(padded_sequence, lengths = lengths, enforce_sorted = False)\n",
    "        # print(padded_sequence.shape)\n",
    "        \n",
    "        packed_out, _ = self.lstm(packed_sequence)\n",
    "        padded_out, _ = pad_packed_sequence(packed_out)\n",
    "    \n",
    "        # print(padded_out.shape)\n",
    "    \n",
    "        out_forward = padded_out[lengths - 1, range(padded_out.shape[1]), :self.hidden_size]\n",
    "        out_reverse = padded_out[0, :, self.hidden_size:]\n",
    "        \n",
    "        # print(out_forward.shape)\n",
    "        # print(out_reverse.shape)\n",
    "        \n",
    "        out = torch.cat([out_forward, out_reverse], dim = 1)\n",
    "        \n",
    "        # print(out.shape)\n",
    "        \n",
    "        x = self.batch_norm(out)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear1(x)\n",
    "        #x = self.relu2(x)\n",
    "        #x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_features = 768 if BERT_TOKENIZER else 300\n",
    "batch_size = 32\n",
    "learning_rate = 0.0001\n",
    "epochs = 50\n",
    "patience = 2\n",
    "class_weight_beta = 0.9999\n",
    "\n",
    "TRANSFORMER_MODEL_NAME = 'roberta-base' # ignore this for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_intent_list(intent_list):\n",
    "    intents = set()\n",
    "    if len(intent_list) == 0:\n",
    "        intents.add('other')\n",
    "    for intent in intent_list:\n",
    "        if intent.startswith('Restaurant'):\n",
    "            intents.add(intent)\n",
    "        elif intent.startswith('Hotel'):\n",
    "            intents.add(intent)\n",
    "        elif intent.startswith('general'):\n",
    "            intents.add(intent)\n",
    "        else:\n",
    "            intents.add('other')\n",
    "    # print(f'Original {intent_list}')\n",
    "    # print(f'Modified {list(intents)}')\n",
    "    return list(intents)\n",
    "\n",
    "def preprocess_split(dataset, split):\n",
    "    df = dataset[split].to_pandas()\n",
    "    new_df = pd.DataFrame(columns = df.columns)\n",
    "    for i in range(len(df)):\n",
    "        # Taken from notebook, to know which lines to skip\n",
    "        row = df.loc[i]\n",
    "        if not any(set(row.turns['frames'][turn_id]['service']).intersection(['hotel', 'restaurant']) for turn_id,utt in enumerate(row.turns['utterance'])):\n",
    "            continue\n",
    "        new_df.loc[len(new_df)] = row\n",
    "        # new_df.loc[len(new_df) - 1]['services'] = process_service_list(new_df.loc[len(new_df) - 1]['services'])\n",
    "        # for i, frame_service in [frame['service'] for frame in df.loc[i].turns['frames']]:\n",
    "            # df.loc[i].turns['frames']\n",
    "    return new_df\n",
    "\n",
    "def extract_feature_df(dataset):\n",
    "    act_types = []\n",
    "    utterance_list = []\n",
    "    embedding_list = []\n",
    "    \n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        turns = dataset.loc[i].turns\n",
    "        # print(dataset.loc[i].turns['utterance'])\n",
    "        # print([frame['service'] for frame in dataset.loc[i].turns['frames']])\n",
    "        for utterance, speaker, dialogue_act in zip(turns['utterance'], turns['speaker'], turns['dialogue_acts']):\n",
    "            if speaker == 0: # if it's the user's turn\n",
    "                if not BERT_TOKENIZER:\n",
    "                    utterance = utterance.translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation)))\n",
    "                act_type = dialogue_act['dialog_act']['act_type']\n",
    "                new_utterance = []\n",
    "                word_embedding_list = []\n",
    "                \n",
    "                #### Spacy tokenization + fasttext\n",
    "                if not BERT_TOKENIZER:\n",
    "                    doc = nlp(utterance)\n",
    "                    for token in doc:\n",
    "                        new_utterance.append(token.lemma_)\n",
    "                        word_embedding_list.append(embedder.get_word_vector(token.lemma_))\n",
    "                        embedder\n",
    "                    embedding = np.stack(word_embedding_list)\n",
    "                #### Bert embedder\n",
    "                else:\n",
    "                    tokenized = transformer_tokenizer(utterance)\n",
    "                    with torch.no_grad():\n",
    "                        embedding = embedding_matrix[tokenized.input_ids].detach().numpy()\n",
    "                ####\n",
    "                \n",
    "                # np.mean(word_embedding_list, axis = 0)\n",
    "                \n",
    "                new_utterance = ' '.join(new_utterance)\n",
    "                \n",
    "                embedding_list.append(embedding)\n",
    "                act_types.append(process_intent_list(act_type))\n",
    "                utterance_list.append(new_utterance)\n",
    "                    \n",
    "    return utterance_list, embedding_list, act_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: multi_woz_v22/v2.2_active_only\n",
      "Found cached dataset multi_woz_v22 (/home/adrian/.cache/huggingface/datasets/multi_woz_v22/v2.2_active_only/2.2.0/6719c8b21478299411a0c6fdb7137c3ebab2e6425129af831687fb7851c69eb5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "680117e7cd1c4f26ada0dfb49b253e79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6321/6321 [02:55<00:00, 35.95it/s]\n",
      "100%|██████████| 745/745 [00:23<00:00, 31.81it/s]\n",
      "100%|██████████| 745/745 [00:22<00:00, 32.94it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('multi_woz_v22')\n",
    "\n",
    "try:\n",
    "    train\n",
    "    print(\"Dataset already loaded, moving on\")\n",
    "except:\n",
    "    train = preprocess_split(dataset, 'train')\n",
    "    test = preprocess_split(dataset, 'test')\n",
    "    val = preprocess_split(dataset, 'validation')\n",
    "    train_utterance_list, train_embedding_list, train_act_type = extract_feature_df(train)\n",
    "    test_utterance_list, test_embedding_list, test_act_type = extract_feature_df(test)\n",
    "    val_utterance_list, val_embedding_list, val_act_type = extract_feature_df(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer().fit(train_act_type)\n",
    "model = MyLSTM(input_size = nr_features, num_cells = 4, hidden_size = 300, bi = True, out_features = len(mlb.classes_)).cuda()\n",
    "model.train()\n",
    "\n",
    "train_labels = mlb.transform(train_act_type)\n",
    "val_labels = mlb.transform(val_act_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts:\n",
      "[('Hotel-Inform', 11521), ('Hotel-Request', 2107), ('Restaurant-Inform', 11063), ('Restaurant-Request', 2719), ('general-bye', 2052), ('general-greet', 78), ('general-thank', 5247), ('other', 13894)]\n",
      "Class weights:\n",
      "[('Hotel-Inform', 0.07781679375321442), ('Hotel-Request', 0.28017169940200043), ('Restaurant-Inform', 0.07953861567289042), ('Restaurant-Request', 0.22358140714587124), ('general-bye', 0.28691842694505404), ('general-greet', 6.8506983696367305), ('general-thank', 0.1303759509443874), ('other', 0.07089873649985191)]\n"
     ]
    }
   ],
   "source": [
    "samples_per_class = [0] * len(mlb.classes_)\n",
    "for act_types in train_act_type:\n",
    "    for act_type in act_types:\n",
    "        samples_per_class[np.argmax(mlb.transform([[act_type]]))] += 1\n",
    "\n",
    "print(\"Class counts:\")\n",
    "print([*zip(mlb.classes_, samples_per_class)])\n",
    "\n",
    "samples_per_class = np.array(samples_per_class)\n",
    "\n",
    "effective_num = 1.0 - np.power(class_weight_beta, samples_per_class)\n",
    "class_weights = (1.0 - class_weight_beta) / effective_num\n",
    "class_weights = class_weights / np.sum(class_weights) * len(mlb.classes_)\n",
    "print(\"Class weights:\")\n",
    "print([*zip(mlb.classes_, class_weights)])\n",
    "class_weights = torch.Tensor(class_weights).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify_tokens_tags(embedding_list, labels_list, batch_size):\n",
    "    embeddings_batch = []\n",
    "    labels_batch = []\n",
    "    \n",
    "    if labels_list is None:\n",
    "        labels_list = range(len(embedding_list))\n",
    "    \n",
    "    for embeddings, label in zip(embedding_list, labels_list):\n",
    "        embeddings_batch.append(torch.Tensor(embeddings).cuda())\n",
    "        labels_batch.append(label)\n",
    "        \n",
    "        if len(embeddings_batch) == batch_size:\n",
    "            yield embeddings_batch, torch.Tensor(labels_batch).cuda()\n",
    "            embeddings_batch.clear()\n",
    "            labels_batch.clear()\n",
    "    \n",
    "    yield embeddings_batch, torch.Tensor(labels_batch).cuda()\n",
    "    return None\n",
    "\n",
    "def compute_loss(model, embedding_list, labels_list, batch_size, criterion):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for embeddings_batch, labels_batch in batchify_tokens_tags(embedding_list, labels_list, batch_size):\n",
    "            out = model.forward(embeddings_batch)\n",
    "            \n",
    "            loss = criterion(out, labels_batch)\n",
    "            losses.append(loss.item())\n",
    "    model.train()\n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7438/2586959704.py:13: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  yield embeddings_batch, torch.Tensor(labels_batch).cuda()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss = 0.09272245624539698, Val loss = 0.03682244825305531\n",
      "Epoch 2: Train loss = 0.03295384650638941, Val loss = 0.02632486456439146\n",
      "Epoch 3: Train loss = 0.02519657322486561, Val loss = 0.021405596352949\n",
      "Epoch 4: Train loss = 0.02166349779718228, Val loss = 0.019664503922119984\n",
      "Epoch 5: Train loss = 0.01978758132012363, Val loss = 0.01953527335468577\n",
      "Epoch 6: Train loss = 0.018533735174435975, Val loss = 0.019357494958444854\n",
      "Epoch 7: Train loss = 0.017745214490370836, Val loss = 0.01800608294321358\n",
      "Epoch 8: Train loss = 0.017017841143676667, Val loss = 0.0170271099874369\n",
      "Epoch 9: Train loss = 0.016363109737717905, Val loss = 0.017018673203124033\n",
      "Epoch 10: Train loss = 0.015827872395624703, Val loss = 0.016353308732585354\n",
      "Epoch 11: Train loss = 0.015377366489030982, Val loss = 0.017076844750846948\n",
      "Epoch 12: Train loss = 0.015201997834286397, Val loss = 0.016352586998753454\n",
      "Epoch 13: Train loss = 0.014838089800944459, Val loss = 0.015820029158227517\n",
      "Epoch 14: Train loss = 0.014489859292822156, Val loss = 0.015494654644589398\n",
      "Epoch 15: Train loss = 0.014210369714338061, Val loss = 0.015302357959782303\n",
      "Epoch 16: Train loss = 0.013920142273704765, Val loss = 0.01553964974276351\n",
      "Epoch 17: Train loss = 0.013616470558414048, Val loss = 0.015310824527122204\n",
      "Epoch 18: Train loss = 0.013361519203240354, Val loss = 0.015249725203518628\n",
      "Epoch 19: Train loss = 0.013292901268503342, Val loss = 0.015067493860759047\n",
      "Epoch 20: Train loss = 0.013018742571819012, Val loss = 0.014972652496451008\n",
      "Epoch 21: Train loss = 0.012949795962321733, Val loss = 0.014914054995999564\n",
      "Epoch 22: Train loss = 0.012711864973512068, Val loss = 0.016895791228850324\n",
      "Epoch 23: Train loss = 0.0125527036140215, Val loss = 0.015039424329625638\n",
      "Epoch 24: Train loss = 0.012404183394733462, Val loss = 0.014747141046161398\n",
      "Epoch 25: Train loss = 0.012184438166564966, Val loss = 0.01498732895806473\n",
      "Epoch 26: Train loss = 0.012238025707719515, Val loss = 0.015233745520615447\n",
      "Epoch 27: Train loss = 0.012073188337436279, Val loss = 0.01507140295477121\n",
      "Epoch 28: Train loss = 0.011896976152065537, Val loss = 0.014763007070406118\n",
      "Epoch 29: Train loss = 0.011819298410178137, Val loss = 0.014328481623059067\n",
      "Epoch 30: Train loss = 0.011648827714178264, Val loss = 0.014416181144062738\n",
      "Epoch 31: Train loss = 0.011561431593746832, Val loss = 0.015221870478592525\n",
      "Epoch 32: Train loss = 0.011470404232614128, Val loss = 0.015088053950165187\n",
      "Epoch 33: Train loss = 0.011831075517921162, Val loss = 0.015246052274387844\n",
      "Epoch 34: Train loss = 0.01124525523836225, Val loss = 0.014908513125001313\n",
      "Epoch 35: Train loss = 0.011198849863094386, Val loss = 0.014663717623295788\n",
      "Epoch 36: Train loss = 0.011134942931086658, Val loss = 0.014988977788788015\n",
      "Epoch 37: Train loss = 0.011143386750787694, Val loss = 0.014862224392757561\n",
      "Epoch 38: Train loss = 0.010927140387555575, Val loss = 0.014906146747538644\n",
      "Epoch 39: Train loss = 0.0109019651810649, Val loss = 0.014968363901353261\n",
      "Epoch 40: Train loss = 0.01083715475592294, Val loss = 0.014884732219838223\n",
      "Epoch 41: Train loss = 0.010770394623067553, Val loss = 0.015024258731993029\n",
      "Epoch 42: Train loss = 0.011075770230567104, Val loss = 0.014638154534623027\n",
      "Epoch 43: Train loss = 0.010554032604139838, Val loss = 0.014642407662281509\n",
      "Epoch 44: Train loss = 0.010482320616147922, Val loss = 0.015019865348955708\n",
      "Epoch 45: Train loss = 0.010437085127390338, Val loss = 0.015459493263135792\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "optim = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "criterion = nn.BCEWithLogitsLoss(weight = class_weights)\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "waited = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_train_loss = []\n",
    "    \n",
    "    for embeddings_batch, labels_batch in batchify_tokens_tags(train_embedding_list, train_labels, batch_size):\n",
    "        optim.zero_grad()\n",
    "        out = model.forward(embeddings_batch)\n",
    "        \n",
    "        # logits_final, labels_final = outputs_keep_useful_part(out.logits, labels_batch, useful_pos_batch)\n",
    "        loss = criterion(out, labels_batch)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        epoch_train_loss.append(loss.item())\n",
    "    \n",
    "    epoch_train_loss = np.mean(epoch_train_loss)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    epoch_val_loss = compute_loss(model, val_embedding_list, val_labels, batch_size, criterion)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}: Train loss = {epoch_train_loss}, Val loss = {epoch_val_loss}\")\n",
    "    \n",
    "    if len(val_losses) != 0 and val_losses[-1] <= epoch_val_loss:\n",
    "        waited += 1\n",
    "        if waited > patience:\n",
    "                val_losses.append(epoch_val_loss)\n",
    "                break\n",
    "    else:\n",
    "        waited = 0\n",
    "    \n",
    "    val_losses.append(epoch_val_loss)\n",
    "\n",
    "plt.plot(train_losses)\n",
    "plt.plot(val_losses)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, embeddings_list, batch_size):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for embeddings_batch, _ in batchify_tokens_tags(embeddings_list, None, batch_size):\n",
    "            out = model.forward(embeddings_batch)\n",
    "            predictions.append((out > 0.5).cpu().detach().numpy())\n",
    "    return np.concatenate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "      Hotel-Inform      0.936     0.822     0.875      1328\n",
      "     Hotel-Request      0.867     0.267     0.408       292\n",
      " Restaurant-Inform      0.940     0.815     0.873      1322\n",
      "Restaurant-Request      0.687     0.322     0.438       286\n",
      "       general-bye      0.991     0.964     0.977       225\n",
      "     general-greet      1.000     0.667     0.800         6\n",
      "     general-thank      0.974     0.965     0.970       693\n",
      "             other      0.961     0.808     0.878      2039\n",
      "\n",
      "         micro avg      0.945     0.788     0.859      6191\n",
      "         macro avg      0.919     0.704     0.777      6191\n",
      "      weighted avg      0.937     0.788     0.848      6191\n",
      "       samples avg      0.821     0.808     0.810      6191\n",
      "\n",
      "acc = 0.7769523151347616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adrian/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "test_act_type_pred = predict(model, test_embedding_list, batch_size)\n",
    "\n",
    "# print(mlb.transform(test_act_type).shape)\n",
    "# print(test_act_type_pred.shape)\n",
    "\n",
    "acc = accuracy_score(mlb.transform(test_act_type), test_act_type_pred)\n",
    "report = classification_report(mlb.transform(test_act_type), test_act_type_pred, target_names = mlb.classes_, digits = 3)\n",
    "print(report)\n",
    "print(f'acc = {acc}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
