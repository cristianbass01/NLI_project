{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import RobertaForTokenClassification\n",
    "from datasets import load_dataset\n",
    "from torch import nn\n",
    "import spacy\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import f_classif, SelectKBest\n",
    "import string\n",
    "import fasttext\n",
    "from sklearn.svm import SVC\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "\n",
    "embedder = fasttext.load_model('fasttext/cc.en.300.bin')\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input, epochs = 100, batch_size = 64, patience = 2, lr = 1e-3):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.BatchNorm1d(input),\n",
    "            nn.Linear(input, input),\n",
    "            nn.Dropout(),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input, 100),\n",
    "            nn.Dropout(),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 50),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.patience = patience\n",
    "        self.lr = lr\n",
    "    \n",
    "    def compute_loss(self, X, y, criterion):\n",
    "        self.eval()\n",
    "        batch_size = self.batch_size\n",
    "        N = X.shape[0]\n",
    "        batches = [(X[(i - batch_size) : (i if i < N else N - 1), :], y[(i - batch_size) : (i if i < N else N - 1)]) for i in range(batch_size, N + batch_size, batch_size)]\n",
    "        with torch.no_grad():\n",
    "            losses = []\n",
    "            for batch, y_true in batches:\n",
    "                y_pred = self.forward(batch)\n",
    "                loss = criterion(y_pred, y_true)\n",
    "                losses.append(loss.item())\n",
    "        self.train()\n",
    "        return np.mean(losses)\n",
    "    \n",
    "    def fit(self, X, y, X_val, y_val):\n",
    "        X = torch.Tensor(X).cuda()\n",
    "        y = torch.Tensor(y).cuda()\n",
    "        X_val = torch.Tensor(X_val).cuda()\n",
    "        y_val = torch.Tensor(y_val).cuda()\n",
    "\n",
    "        self.head = nn.Linear(50, y.shape[1]).cuda()\n",
    "        batch_size = self.batch_size\n",
    "        optim = torch.optim.Adam(self.parameters(), lr = self.lr)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        N = X.shape[0]\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        waited = 0\n",
    "        \n",
    "        for epoch in tqdm(range(self.epochs)):\n",
    "            batches = [(X[(i - batch_size) : (i if i < N else N - 1), :], y[(i - batch_size) : (i if i < N else N - 1)]) for i in range(batch_size, N + batch_size, batch_size)]\n",
    "            epoch_train_loss = []\n",
    "            for batch, y_true in batches:\n",
    "                y_pred = self.forward(batch)\n",
    "                loss = criterion(y_pred, y_true)\n",
    "                optim.zero_grad()\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "                epoch_train_loss.append(loss.item())\n",
    "            \n",
    "            epoch_train_loss = np.mean(epoch_train_loss)\n",
    "            train_losses.append(epoch_train_loss)\n",
    "            \n",
    "            epoch_val_loss = self.compute_loss(X_val, y_val, criterion)\n",
    "            if len(val_losses) != 0 and val_losses[-1] <= epoch_val_loss:\n",
    "                waited += 1\n",
    "                if waited > self.patience:\n",
    "                    break\n",
    "            else:\n",
    "                waited = 0\n",
    "\n",
    "            val_losses.append(epoch_val_loss)\n",
    "            \n",
    "            \n",
    "        return train_losses, val_losses\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.head(self.mlp(X))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = torch.Tensor(X).cuda()\n",
    "        y = self.forward(X)\n",
    "        return (y > 0.5).float().cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self, num_cells, hidden_size, bi, out_features):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size = 300, num_layers = num_cells, hidden_size = hidden_size, bidirectional=bi, proj_size = 1, batch_first = True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear = nn.Linear(in_features = 187 * 2 if bi else 187, out_features = out_features)\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            self.linear.bias.fill_(-torch.log(torch.tensor(out_features - 1)))\n",
    "    \n",
    "    def forward(self, embedding_sequence):\n",
    "        padded_sequence = pad_sequence(embedding_sequence)\n",
    "        packed_sequence = pack_padded_sequence(padded_sequence)\n",
    "        \n",
    "        packed_out, _ = self.lstm(packed_sequence)\n",
    "        padded_out = pad_packed_sequence(packed_out)\n",
    "        \n",
    "        print(padded_out.shape)\n",
    "        \n",
    "        x = self.relu(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(300).cuda() # Replace model instantiation with another class here (SVC for example) if wishing to test other models\n",
    "nr_features = 300 # the number of top-scoring features that will be selected ranked by ANOVA score\n",
    "\n",
    "TRANSFORMER_MODEL_NAME = 'roberta-base' # ignore this for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_intent_list(intent_list):\n",
    "    intents = set()\n",
    "    if len(intent_list) == 0:\n",
    "        intents.add('other')\n",
    "    for intent in intent_list:\n",
    "        if intent.startswith('Restaurant'):\n",
    "            intents.add(intent)\n",
    "        elif intent.startswith('Hotel'):\n",
    "            intents.add(intent)\n",
    "        elif intent.startswith('general'):\n",
    "            intents.add(intent)\n",
    "        else:\n",
    "            intents.add('other')\n",
    "    # print(f'Original {intent_list}')\n",
    "    # print(f'Modified {list(intents)}')\n",
    "    return list(intents)\n",
    "\n",
    "def preprocess_split(dataset, split):\n",
    "    df = dataset[split].to_pandas()\n",
    "    new_df = pd.DataFrame(columns = df.columns)\n",
    "    for i in range(len(df)):\n",
    "        # Taken from notebook, to know which lines to skip\n",
    "        row = df.loc[i]\n",
    "        if not any(set(row.turns['frames'][turn_id]['service']).intersection(['hotel', 'restaurant']) for turn_id,utt in enumerate(row.turns['utterance'])):\n",
    "            continue\n",
    "        new_df.loc[len(new_df)] = row\n",
    "        # new_df.loc[len(new_df) - 1]['services'] = process_service_list(new_df.loc[len(new_df) - 1]['services'])\n",
    "        # for i, frame_service in [frame['service'] for frame in df.loc[i].turns['frames']]:\n",
    "            # df.loc[i].turns['frames']\n",
    "    return new_df\n",
    "\n",
    "def extract_feature_df(dataset):\n",
    "    act_types = []\n",
    "    utterance_list = []\n",
    "    embedding_list = []\n",
    "    \n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        turns = dataset.loc[i].turns\n",
    "        # print(dataset.loc[i].turns['utterance'])\n",
    "        # print([frame['service'] for frame in dataset.loc[i].turns['frames']])\n",
    "        for utterance, speaker, dialogue_act in zip(turns['utterance'], turns['speaker'], turns['dialogue_acts']):\n",
    "            if speaker == 0: # if it's the user's turn\n",
    "                utterance = utterance.translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation)))\n",
    "                act_type = dialogue_act['dialog_act']['act_type']\n",
    "                new_utterance = []\n",
    "                word_embedding_list = []\n",
    "                \n",
    "                doc = nlp(utterance)\n",
    "                for token in doc:\n",
    "                    new_utterance.append(token.lemma_)\n",
    "                    word_embedding_list.append(embedder.get_word_vector(token.lemma_))\n",
    "                \n",
    "                # np.mean(word_embedding_list, axis = 0)\n",
    "                embedding = np.stack(word_embedding_list)\n",
    "                \n",
    "                new_utterance = ' '.join(new_utterance)\n",
    "                \n",
    "                embedding_list.append(embedding)\n",
    "                act_types.append(process_intent_list(act_type))\n",
    "                utterance_list.append(new_utterance)\n",
    "    \n",
    "    tf_idf = TfidfVectorizer().fit(utterance_list)\n",
    "                    \n",
    "    return tf_idf, utterance_list, embedding_list, act_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: multi_woz_v22/v2.2_active_only\n",
      "Found cached dataset multi_woz_v22 (/home/adrian/.cache/huggingface/datasets/multi_woz_v22/v2.2_active_only/2.2.0/6719c8b21478299411a0c6fdb7137c3ebab2e6425129af831687fb7851c69eb5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a7385ce8abc4fbcaf509190d9119787",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6321/6321 [03:45<00:00, 28.08it/s]\n",
      "100%|██████████| 745/745 [00:28<00:00, 25.70it/s]\n",
      "100%|██████████| 745/745 [00:29<00:00, 25.18it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('multi_woz_v22')\n",
    "\n",
    "try:\n",
    "    train\n",
    "    print(\"Dataset already loaded, moving on\")\n",
    "except:\n",
    "    train = preprocess_split(dataset, 'train')\n",
    "    test = preprocess_split(dataset, 'test')\n",
    "    val = preprocess_split(dataset, 'validation')\n",
    "    tf_idf, train_utterance_list, train_embedding_list, train_act_type = extract_feature_df(train)\n",
    "    _, test_utterance_list, test_embedding_list, test_act_type = extract_feature_df(test)\n",
    "    _, val_utterance_list, val_embedding_list, val_act_type = extract_feature_df(test)\n",
    "    del embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (2072393957.py, line 29)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_6223/2072393957.py\"\u001b[0;36m, line \u001b[0;32m29\u001b[0m\n\u001b[0;31m    else\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected ':'\n"
     ]
    }
   ],
   "source": [
    "if not isinstance(model, MLP):\n",
    "    model = MultiOutputClassifier(model)\n",
    "mlb = MultiLabelBinarizer().fit(train_act_type)\n",
    "\n",
    "train_tf_idf_feats = tf_idf.transform(train_utterance_list)\n",
    "train_feats = np.concatenate([train_tf_idf_feats.toarray(), np.stack(train_embedding_list)], axis = 1)\n",
    "print(train_feats.shape)\n",
    "# train_feats = np.stack(train_embedding_list) UNCOMMENT TO ONLY USE EMBEDDINGS\n",
    "\n",
    "train_labels = mlb.transform(train_act_type)\n",
    "print(train_labels)\n",
    "print(train_labels[0])\n",
    "feature_scores = [] \n",
    "for i in range(train_labels.shape[1]):\n",
    "    selector = SelectKBest(f_classif, k='all')\n",
    "    selector.fit(train_feats, train_labels[:, i])\n",
    "    feature_scores.append(list(selector.scores_))\n",
    "feature_scores = np.mean(feature_scores, axis=0)\n",
    "selected_features = np.argpartition(feature_scores, -nr_features)[-nr_features:]\n",
    "train_feats_selected = train_feats[:, selected_features]\n",
    "\n",
    "print(f'{train_feats_selected.shape[1]} from {train_feats.shape[1]} features selected')\n",
    "\n",
    "val_tf_idf_feats = tf_idf.transform(val_utterance_list)\n",
    "val_feats = np.concatenate([val_tf_idf_feats.toarray(), np.stack(val_embedding_list)], axis = 1)\n",
    "val_feats_selected = val_feats[:, selected_features]\n",
    "val_labels = mlb.transform(val_act_type)\n",
    "\n",
    "if not isinstance(model, MLP):\n",
    "    model.fit(train_feats_selected, train_labels)\n",
    "else:\n",
    "    train_losses, val_losses = model.fit(train_feats_selected, train_labels, val_feats_selected, val_labels)\n",
    "    plt.plot(train_losses)\n",
    "    plt.plot(val_losses)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc = 0.7963026952315134, precision = 0.9192140506084714, recall = 0.703828673485707, f_score = 0.7783417241481402\n"
     ]
    }
   ],
   "source": [
    "test_tf_idf_feats = tf_idf.transform(test_utterance_list)\n",
    "test_feats = np.concatenate([test_tf_idf_feats.toarray(), np.stack(test_embedding_list)], axis = 1)\n",
    "# test_feats = np.stack(test_embedding_list) UNCOMMENT TO ONLY USE EMBEDDINGS\n",
    "test_feats_selected = test_feats[:, selected_features]\n",
    "\n",
    "if isinstance(model, MLP):\n",
    "    model.eval()\n",
    "test_act_type_pred = model.predict(test_feats_selected)\n",
    "\n",
    "acc = accuracy_score(mlb.transform(test_act_type), test_act_type_pred)\n",
    "precision, recall, f_score, _ = precision_recall_fscore_support(mlb.transform(test_act_type), test_act_type_pred, average = 'macro')\n",
    "print(f'acc = {acc}, precision = {precision}, recall = {recall}, f_score = {f_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roberta TEST\n",
    "(we will keep this code for now as reference and we will leverage it later when we will use transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, base_model_name, num_classes):\n",
    "        super().__init__()\n",
    "        self.transformer = RobertaModel.from_pretrained(base_model_name)\n",
    "        self.pre_classifier = nn.Linear(768, 768)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(768, num_classes)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        cls = self.transformer(**input).last_hidden_state[:, 0, :]\n",
    "        x = self.pre_classifier(cls)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RobertaModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6085/3414135274.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRANSFORMER_MODEL_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformerClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRANSFORMER_MODEL_NAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"We love doing NLI!\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_6085/682487097.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, base_model_name, num_classes)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_model_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRobertaModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m768\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m768\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RobertaModel' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "classifier = TransformerClassifier(TRANSFORMER_MODEL_NAME, 2)\n",
    "\n",
    "\n",
    "inputs = tokenizer(\"We love doing NLI!\", return_tensors=\"pt\")\n",
    "output = classifier(inputs)\n",
    "print(output.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
