{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import RobertaForTokenClassification\n",
    "from datasets import load_dataset\n",
    "from torch import nn\n",
    "import spacy\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import f_classif, SelectKBest\n",
    "import string\n",
    "import fasttext\n",
    "from sklearn.svm import SVC\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "\n",
    "embedder = fasttext.load_model('fasttext/cc.en.300.bin')\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input, epochs = 100, batch_size = 64, patience = 2, lr = 1e-3):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.BatchNorm1d(input),\n",
    "            nn.Linear(input, input),\n",
    "            nn.Dropout(),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input, 100),\n",
    "            nn.Dropout(),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 50),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.patience = patience\n",
    "        self.lr = lr\n",
    "    \n",
    "    def compute_loss(self, X, y, criterion):\n",
    "        self.eval()\n",
    "        batch_size = self.batch_size\n",
    "        N = X.shape[0]\n",
    "        batches = [(X[(i - batch_size) : (i if i < N else N - 1), :], y[(i - batch_size) : (i if i < N else N - 1)]) for i in range(batch_size, N + batch_size, batch_size)]\n",
    "        with torch.no_grad():\n",
    "            losses = []\n",
    "            for batch, y_true in batches:\n",
    "                y_pred = self.forward(batch)\n",
    "                loss = criterion(y_pred, y_true)\n",
    "                losses.append(loss.item())\n",
    "        self.train()\n",
    "        return np.mean(losses)\n",
    "    \n",
    "    def fit(self, X, y, X_val, y_val):\n",
    "        X = torch.Tensor(X).cuda()\n",
    "        y = torch.Tensor(y).cuda()\n",
    "        X_val = torch.Tensor(X_val).cuda()\n",
    "        y_val = torch.Tensor(y_val).cuda()\n",
    "\n",
    "        self.head = nn.Linear(50, y.shape[1]).cuda()\n",
    "        batch_size = self.batch_size\n",
    "        optim = torch.optim.Adam(self.parameters(), lr = self.lr)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        N = X.shape[0]\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        waited = 0\n",
    "        \n",
    "        for epoch in tqdm(range(self.epochs)):\n",
    "            batches = [(X[(i - batch_size) : (i if i < N else N - 1), :], y[(i - batch_size) : (i if i < N else N - 1)]) for i in range(batch_size, N + batch_size, batch_size)]\n",
    "            epoch_train_loss = []\n",
    "            for batch, y_true in batches:\n",
    "                y_pred = self.forward(batch)\n",
    "                loss = criterion(y_pred, y_true)\n",
    "                optim.zero_grad()\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "                epoch_train_loss.append(loss.item())\n",
    "            \n",
    "            epoch_train_loss = np.mean(epoch_train_loss)\n",
    "            train_losses.append(epoch_train_loss)\n",
    "            \n",
    "            epoch_val_loss = self.compute_loss(X_val, y_val, criterion)\n",
    "            if len(val_losses) != 0 and val_losses[-1] <= epoch_val_loss:\n",
    "                waited += 1\n",
    "                if waited > self.patience:\n",
    "                    break\n",
    "            else:\n",
    "                waited = 0\n",
    "\n",
    "            val_losses.append(epoch_val_loss)\n",
    "            \n",
    "            \n",
    "        return train_losses, val_losses\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.head(self.mlp(X))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = torch.Tensor(X).cuda()\n",
    "        y = self.forward(X)\n",
    "        return (y > 0.5).float().cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(300).cuda() # Replace model instantiation with another class here (SVC for example) if wishing to test other models\n",
    "nr_features = 300 # the number of top-scoring features that will be selected ranked by ANOVA score\n",
    "use_history = True # whether to use the history of the previous sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_intent_list(intent_list):\n",
    "    intents = set()\n",
    "    if len(intent_list) == 0:\n",
    "        intents.add('other')\n",
    "    for intent in intent_list:\n",
    "        if intent.startswith('Restaurant'):\n",
    "            intents.add(intent)\n",
    "        elif intent.startswith('Hotel'):\n",
    "            intents.add(intent)\n",
    "        elif intent.startswith('general'):\n",
    "            intents.add(intent)\n",
    "        else:\n",
    "            intents.add('other')\n",
    "    # print(f'Original {intent_list}')\n",
    "    # print(f'Modified {list(intents)}')\n",
    "    return list(intents)\n",
    "\n",
    "def preprocess_split(dataset, split):\n",
    "    df = dataset[split].to_pandas()\n",
    "    new_df = pd.DataFrame(columns = df.columns)\n",
    "    for i in range(len(df)):\n",
    "        # Taken from notebook, to know which lines to skip\n",
    "        row = df.loc[i]\n",
    "        if not any(set(row.turns['frames'][turn_id]['service']).intersection(['hotel', 'restaurant']) for turn_id,utt in enumerate(row.turns['utterance'])):\n",
    "            continue\n",
    "        new_df.loc[len(new_df)] = row\n",
    "        # new_df.loc[len(new_df) - 1]['services'] = process_service_list(new_df.loc[len(new_df) - 1]['services'])\n",
    "        # for i, frame_service in [frame['service'] for frame in df.loc[i].turns['frames']]:\n",
    "            # df.loc[i].turns['frames']\n",
    "    return new_df\n",
    "\n",
    "def extract_feature_df(dataset):\n",
    "    act_types = []\n",
    "    utterance_list = []\n",
    "    embedding_list = []\n",
    "    \n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        turns = dataset.loc[i].turns\n",
    "        # print(dataset.loc[i].turns['utterance'])\n",
    "        # print([frame['service'] for frame in dataset.loc[i].turns['frames']])\n",
    "        for j, (utterance, speaker, dialogue_act) in enumerate(zip(turns['utterance'], turns['speaker'], turns['dialogue_acts'])):\n",
    "            if j == 0:\n",
    "                prev_user_utterance = ''\n",
    "                prev_user_acts = []\n",
    "                prev_bot_utterance = ''\n",
    "                prev_bot_acts = []\n",
    "            else:\n",
    "                prev_user_utterance = turns['utterance'][j - 2]\n",
    "                prev_user_acts = turns['dialogue_acts'][j - 2]['dialog_act']['act_type']\n",
    "                prev_bot_utterance = turns['utterance'][j - 1]\n",
    "                prev_bot_acts = turns['dialogue_acts'][j - 1]['dialog_act']['act_type']\n",
    "            \n",
    "            if speaker == 0: # if it's the user's turn\n",
    "                if use_history:\n",
    "                    composed_utterance = ' | '.join([prev_user_utterance, ', '.join(prev_user_acts), prev_bot_utterance, ', '.join(prev_bot_acts), utterance])\n",
    "                else:\n",
    "                    composed_utterance = utterance\n",
    "                # print(composed_utterance)\n",
    "                \n",
    "                # utterance = composed_utterance.translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation)))\n",
    "                utterance = composed_utterance\n",
    "                act_type = dialogue_act['dialog_act']['act_type']\n",
    "                new_utterance = []\n",
    "                word_embedding_list = []\n",
    "                \n",
    "                # doc = nlp(utterance)\n",
    "                # for token in doc:\n",
    "                #     new_utterance.append(token.lemma_)\n",
    "                #     word_embedding_list.append(embedder.get_word_vector(token.lemma_))\n",
    "                \n",
    "                # embedding = np.mean(word_embedding_list, axis = 0)\n",
    "                # print(composed_utterance)\n",
    "                embedding = embedder.get_sentence_vector(composed_utterance)\n",
    "                # embedding = np.stack(word_embedding_list)\n",
    "                \n",
    "                new_utterance = ' '.join(new_utterance)\n",
    "                \n",
    "                embedding_list.append(embedding)\n",
    "                act_types.append(process_intent_list(act_type))\n",
    "                utterance_list.append(utterance)\n",
    "    \n",
    "    tf_idf = TfidfVectorizer().fit(utterance_list)\n",
    "                    \n",
    "    return tf_idf, utterance_list, embedding_list, act_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: multi_woz_v22/v2.2_active_only\n",
      "Found cached dataset multi_woz_v22 (/home/adrian/.cache/huggingface/datasets/multi_woz_v22/v2.2_active_only/2.2.0/6719c8b21478299411a0c6fdb7137c3ebab2e6425129af831687fb7851c69eb5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9fdd5dabf2a42258ca717da157f4e30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6321/6321 [00:01<00:00, 3703.61it/s]\n",
      "100%|██████████| 745/745 [00:00<00:00, 3593.78it/s]\n",
      "100%|██████████| 745/745 [00:00<00:00, 3602.49it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('multi_woz_v22')\n",
    "\n",
    "try:\n",
    "    train\n",
    "    print(\"Dataset already loaded, moving on\")\n",
    "except:\n",
    "    train = preprocess_split(dataset, 'train')\n",
    "    test = preprocess_split(dataset, 'test')\n",
    "    val = preprocess_split(dataset, 'validation')\n",
    "    tf_idf, train_utterance_list, train_embedding_list, train_act_type = extract_feature_df(train)\n",
    "    _, test_utterance_list, test_embedding_list, test_act_type = extract_feature_df(test)\n",
    "    _, val_utterance_list, val_embedding_list, val_act_type = extract_feature_df(test)\n",
    "    del embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45794, 12668)\n",
      "[[0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "[0 0 1 0 0 0 0 0]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "if not isinstance(model, MLP):\n",
    "    model = MultiOutputClassifier(model)\n",
    "mlb = MultiLabelBinarizer().fit(train_act_type)\n",
    "\n",
    "train_tf_idf_feats = tf_idf.transform(train_utterance_list)\n",
    "train_feats = np.concatenate([train_tf_idf_feats.toarray(), np.stack(train_embedding_list)], axis = 1)\n",
    "print(train_feats.shape)\n",
    "# train_feats = np.stack(train_embedding_list) UNCOMMENT TO ONLY USE EMBEDDINGS\n",
    "\n",
    "train_labels = mlb.transform(train_act_type)\n",
    "print(train_labels)\n",
    "print(train_labels[0])\n",
    "feature_scores = [] \n",
    "for i in range(train_labels.shape[1]):\n",
    "    selector = SelectKBest(f_classif, k='all')\n",
    "    selector.fit(train_feats, train_labels[:, i])\n",
    "    feature_scores.append(list(selector.scores_))\n",
    "feature_scores = np.mean(feature_scores, axis=0)\n",
    "selected_features = np.argpartition(feature_scores, -nr_features)[-nr_features:]\n",
    "train_feats_selected = train_feats[:, selected_features]\n",
    "\n",
    "print(f'{train_feats_selected.shape[1]} from {train_feats.shape[1]} features selected')\n",
    "\n",
    "val_tf_idf_feats = tf_idf.transform(val_utterance_list)\n",
    "val_feats = np.concatenate([val_tf_idf_feats.toarray(), np.stack(val_embedding_list)], axis = 1)\n",
    "val_feats_selected = val_feats[:, selected_features]\n",
    "val_labels = mlb.transform(val_act_type)\n",
    "\n",
    "if not isinstance(model, MLP):\n",
    "    model.fit(train_feats_selected, train_labels)\n",
    "else:\n",
    "    train_losses, val_losses = model.fit(train_feats_selected, train_labels, val_feats_selected, val_labels)\n",
    "    plt.plot(train_losses)\n",
    "    plt.plot(val_losses)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'selected_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18831/3466401294.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtest_feats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_tf_idf_feats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_embedding_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# test_feats = np.stack(test_embedding_list) UNCOMMENT TO ONLY USE EMBEDDINGS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtest_feats_selected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_feats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMLP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'selected_features' is not defined"
     ]
    }
   ],
   "source": [
    "test_tf_idf_feats = tf_idf.transform(test_utterance_list)\n",
    "test_feats = np.concatenate([test_tf_idf_feats.toarray(), np.stack(test_embedding_list)], axis = 1)\n",
    "# test_feats = np.stack(test_embedding_list) UNCOMMENT TO ONLY USE EMBEDDINGS\n",
    "test_feats_selected = test_feats[:, selected_features]\n",
    "\n",
    "if isinstance(model, MLP):\n",
    "    model.eval()\n",
    "test_act_type_pred = model.predict(test_feats_selected)\n",
    "\n",
    "acc = accuracy_score(mlb.transform(test_act_type), test_act_type_pred)\n",
    "precision, recall, f_score, _ = precision_recall_fscore_support(mlb.transform(test_act_type), test_act_type_pred, average = 'macro')\n",
    "print(f'acc = {acc}, precision = {precision}, recall = {recall}, f_score = {f_score}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
